{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import argparse\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from efficientdet.dataset import LabeledDataset, LabeledDataset_coco, Resizer, Normalizer, Augmenter, collater\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from efficientdet.loss import FocalLoss\n",
    "from efficientdet.utils import collate_fn_dl\n",
    "from utils.sync_batchnorm import patch_replication_callback\n",
    "from utils.utils import replace_w_sync_bn, CustomDataParallel, get_last_weights, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, project_file):\n",
    "        self.params = yaml.safe_load(open(project_file).read())\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return self.params.get(item, None)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser('Yet Another EfficientDet Pytorch: SOTA object detection network - Zylo117')\n",
    "    parser.add_argument('-p', '--project', type=str, default='dl2020', help='project file that contains parameters')\n",
    "    parser.add_argument('-c', '--compound_coef', type=int, default=0, help='coefficients of efficientdet')\n",
    "    parser.add_argument('-n', '--num_workers', type=int, default=2, help='num_workers of dataloader')\n",
    "    parser.add_argument('--batch_size', type=int, default=3, help='The number of images per batch among all devices')\n",
    "    parser.add_argument('--head_only', type=bool, default=False,\n",
    "                        help='whether finetunes only the regressor and the classifier, '\n",
    "                             'useful in early stage convergence or small/easy dataset')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--optim', type=str, default='adamw', help='select optimizer for training, '\n",
    "                                                                   'suggest using \\'admaw\\' until the'\n",
    "                                                                   ' very final stage then switch to \\'sgd\\'')\n",
    "    parser.add_argument('--alpha', type=float, default=0.25)\n",
    "    parser.add_argument('--gamma', type=float, default=1.5)\n",
    "    parser.add_argument('--num_epochs', type=int, default=100)\n",
    "    parser.add_argument('--val_interval', type=int, default=1, help='Number of epoches between valing phases')\n",
    "    parser.add_argument('--save_interval', type=int, default=500, help='Number of steps between saving')\n",
    "    parser.add_argument('--es_min_delta', type=float, default=0.0,\n",
    "                        help='Early stopping\\'s parameter: minimum change loss to qualify as an improvement')\n",
    "    parser.add_argument('--es_patience', type=int, default=0,\n",
    "                        help='Early stopping\\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.')\n",
    "    parser.add_argument('--data_path', type=str, default='datasets/', help='the root folder of dataset')\n",
    "    parser.add_argument('--annotation', type=str, default='annotation.csv', help='annotation csv file name')\n",
    "    parser.add_argument('--log_path', type=str, default='logs/')\n",
    "    parser.add_argument('--load_weights', type=str, default=None,\n",
    "                        help='whether to load weights from a checkpoint, set None to initialize, set \\'last\\' to load last checkpoint')\n",
    "    parser.add_argument('--saved_path', type=str, default='saved/')\n",
    "    parser.add_argument('--debug', type=bool, default=False, help='whether visualize the predicted boxes of trainging, '\n",
    "                                                                  'the output images will be in test/')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params(f'projects/dl2020.yml')\n",
    "\n",
    "if params.num_gpus == 0:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = 'saved/' + f'/{params.project_name}/'\n",
    "log_path = 'logs/' + f'/{params.project_name}/tensorboard/'\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "os.makedirs(saved_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {'batch_size': 1,\n",
    "                   'shuffle': False,\n",
    "                   'drop_last': True,\n",
    "                   'collate_fn': collater,\n",
    "#                    'collate_fn': collate_fn_dl,\n",
    "                   'num_workers': 0}\n",
    "\n",
    "val_params = {'batch_size': 2,\n",
    "              'shuffle': False,\n",
    "              'drop_last': True,\n",
    "              'collate_fn': collater,\n",
    "#               'collate_fn': collate_fn_dl,\n",
    "              'num_workers': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try new data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scene_index = np.arange(106, 127)\n",
    "# val_scene_index = np.arange(128, 134)\n",
    "# input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "\n",
    "# transform_dl = torchvision.transforms.ToTensor()\n",
    "\n",
    "# transform = transforms.Compose([Normalizer(mean=params.mean, std=params.std),\\\n",
    "#                                 Augmenter(),\\\n",
    "#                                 Resizer(input_sizes[0])])\n",
    "\n",
    "# training_set = LabeledDataset(image_folder = os.path.join('datasets/', params.project_name),\n",
    "#                                   annotation_file = os.path.join('datasets/', params.project_name, 'annotation.csv'),\n",
    "#                                   scene_index = train_scene_index,\n",
    "#                                   transform = transform_dl,\n",
    "# #                                   transform = transform,\n",
    "#                                   extra_info = True\n",
    "#                                   )\n",
    "\n",
    "# training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "\n",
    "# val_set = LabeledDataset(image_folder = os.path.join('datasets/', params.project_name),\n",
    "#                                   annotation_file = os.path.join('datasets/', params.project_name, 'annotation.csv'),\n",
    "#                                   scene_index = val_scene_index,\n",
    "#                                   transform = transform_dl,\n",
    "#                                   extra_info = True\n",
    "#                                   )\n",
    "\n",
    "# val_generator = DataLoader(val_set, **val_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scene_index = np.arange(106, 127)\n",
    "val_scene_index = np.arange(128, 134)\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "\n",
    "transform = transforms.Compose([Normalizer(mean=params.mean, std=params.std),\\\n",
    "                                Augmenter(),\\\n",
    "                                Resizer(input_sizes[0])])\n",
    "\n",
    "training_set = LabeledDataset_coco(image_folder = os.path.join('datasets/', params.project_name),\n",
    "                                  annotation_file = os.path.join('datasets/', params.project_name, 'annotation_newfeat.csv'),\n",
    "                                  scene_index = train_scene_index,\n",
    "                                  transform = transform,\n",
    "                                  extra_info = True\n",
    "                                  )\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "val_set = LabeledDataset_coco(image_folder = os.path.join('datasets/', params.project_name),\n",
    "                                  annotation_file = os.path.join('datasets/', params.project_name, 'annotation_newfeat.csv'),\n",
    "                                  scene_index = val_scene_index,\n",
    "                                  transform = transform,\n",
    "                                  extra_info = True\n",
    "                                  )\n",
    "\n",
    "val_generator = DataLoader(val_set, **val_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(training_generator)\n",
    "# for iter, data in enumerate(progress_bar):\n",
    "#     imgs = data['img']\n",
    "#     annot = data['annot']\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "# matplotlib.rcParams['figure.dpi'] = 200\n",
    "# # plt.figure(figsize=(3,18))\n",
    "# plt.imshow(torchvision.utils.make_grid(imgs[0], nrow=3).numpy().transpose(1, 2, 0))\n",
    "# plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientDetBackbone(num_classes=len(params.obj_list), compound_coef = 0,\n",
    "                             ratios=eval(params.anchors_ratios), scales=eval(params.anchors_scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] initializing weights...\n"
     ]
    }
   ],
   "source": [
    "load_weights = None\n",
    "if load_weights is not None:\n",
    "    if opt.load_weights.endswith('.pth'):\n",
    "        weights_path = opt.load_weights\n",
    "    else:\n",
    "        weights_path = get_last_weights(opt.saved_path)\n",
    "    try:\n",
    "        last_step = int(os.path.basename(weights_path).split('_')[-1].split('.')[0])\n",
    "    except:\n",
    "        last_step = 0\n",
    "\n",
    "    try:\n",
    "        ret = model.load_state_dict(torch.load(weights_path), strict=False)\n",
    "    except RuntimeError as e:\n",
    "        print(f'[Warning] Ignoring {e}')\n",
    "        print(\n",
    "            '[Warning] Don\\'t panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.')\n",
    "\n",
    "    print(f'[Info] loaded weights: {os.path.basename(weights_path)}, resuming checkpoint from step: {last_step}')\n",
    "else:\n",
    "    last_step = 0\n",
    "    print('[Info] initializing weights...')\n",
    "    init_weights(model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if opt.head_only:\n",
    "#     def freeze_backbone(m):\n",
    "#         classname = m.__class__.__name__\n",
    "#         for ntl in ['EfficientNet', 'BiFPN']:\n",
    "#             if ntl in classname:\n",
    "#                 for param in m.parameters():\n",
    "#                     param.requires_grad = False\n",
    "\n",
    "#     model.apply(freeze_backbone)\n",
    "#     print('[Info] freezed backbone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.num_gpus > 1 and 3 // params.num_gpus < 4:\n",
    "    model.apply(replace_w_sync_bn)\n",
    "    use_sync_bn = True\n",
    "else:\n",
    "    use_sync_bn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_path + f'/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithLoss(nn.Module):\n",
    "    def __init__(self, model, debug=False):\n",
    "        super().__init__()\n",
    "        self.criterion = FocalLoss()\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, imgs, annotations, obj_list=None):\n",
    "        _, regression, classification, anchors = self.model(imgs)\n",
    "        if self.debug:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations,\n",
    "                                                imgs=imgs, obj_list=obj_list)\n",
    "        else:\n",
    "            cls_loss, reg_loss = self.criterion(classification, regression, anchors, annotations)\n",
    "        return cls_loss, reg_loss, regression, classification, anchors\n",
    "\n",
    "model = ModelWithLoss(model, debug= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if params.num_gpus > 0:\n",
    "#     model = model.cuda()\n",
    "#     if params.num_gpus > 1:\n",
    "#         model = CustomDataParallel(model, params.num_gpus)\n",
    "#         if use_sync_bn:\n",
    "#             patch_replication_callback(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = 'adamw'\n",
    "if optim == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "best_loss = 1e5\n",
    "best_epoch = 0\n",
    "step = max(0, last_step)\n",
    "model.train()\n",
    "\n",
    "num_iter_per_epoch = len(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, name):\n",
    "    if isinstance(model, CustomDataParallel):\n",
    "        torch.save(model.module.model.state_dict(), os.path.join('saved/', name))\n",
    "    else:\n",
    "        torch.save(model.model.state_dict(), os.path.join('saved/', name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5845e747070346f7af43be694cbd2a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1323.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------load_annotations---------------------------------\n",
      "After input annotations(106,0) shape: (21, 5)\n",
      "Value\n",
      "[[ 1.57958114e+00 -3.01125490e+00  6.77560550e+00  2.77345391e+00\n",
      "   0.00000000e+00]\n",
      " [ 1.81180619e+01 -3.09096431e+00  1.05814777e+01  2.78493450e+00\n",
      "   0.00000000e+00]\n",
      " [-8.46729368e+00  2.27888324e+01  3.84152349e+00  2.05973015e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.89518236e+01  2.64495094e+01  5.00120193e+00  1.94384072e+00\n",
      "   2.00000000e+00]\n",
      " [-3.48784626e+01  2.67261133e+01  4.33909305e+00  1.88932011e+00\n",
      "   2.00000000e+00]\n",
      " [ 5.25055809e+00  2.63793197e+01  5.01894456e+00  2.08252688e+00\n",
      "   2.00000000e+00]\n",
      " [ 2.67840713e+01 -3.15827703e+00  5.93576901e+00  2.60819039e+00\n",
      "   2.00000000e+00]\n",
      " [-8.05685547e+00 -3.02108314e+00  4.66020817e+00  1.96966549e+00\n",
      "   2.00000000e+00]\n",
      " [-1.51386673e+01  2.64094474e+01  4.35025829e+00  1.96431456e+00\n",
      "   2.00000000e+00]\n",
      " [-2.14512715e+01 -3.11078157e+00  4.96560242e+00  2.08605442e+00\n",
      "   2.00000000e+00]\n",
      " [-1.60810737e+00  2.64716945e+01  5.09524710e+00  1.96505103e+00\n",
      "   2.00000000e+00]\n",
      " [-1.46998493e+01 -3.22831736e+00  6.29920725e+00  2.43933875e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.05023147e+01  6.81878332e+00  4.37913159e+00  1.96694403e+00\n",
      "   2.00000000e+00]\n",
      " [-2.78289321e+01 -3.01415591e+00  5.64180844e+00  2.30348389e+00\n",
      "   0.00000000e+00]\n",
      " [ 1.14088176e+01  2.63736241e+01  5.02643338e+00  2.13292927e+00\n",
      "   2.00000000e+00]\n",
      " [ 2.77546081e+01  2.64294116e+01  4.71212162e+00  1.90518307e+00\n",
      "   2.00000000e+00]\n",
      " [-2.89237825e+01  2.65495109e+01  4.47508921e+00  1.88863744e+00\n",
      "   2.00000000e+00]\n",
      " [-1.12348084e+01  3.65351430e+00  4.61125740e+00  2.06233688e+00\n",
      "   2.00000000e+00]\n",
      " [-2.38718599e+01  2.86119607e-02  4.61031275e+00  2.00536837e+00\n",
      "   2.00000000e+00]\n",
      " [ 9.08817135e+00 -3.17211753e+00  4.84077289e+00  2.13511323e+00\n",
      "   2.00000000e+00]\n",
      " [-2.21930425e+01  2.65014979e+01  4.87609980e+00  1.89656898e+00\n",
      "   2.00000000e+00]]\n",
      "After transform annotations(106,0) shape: (21, 5)\n",
      "Value\n",
      "[[ 1.57958114e+00 -3.01125490e+00  8.35518664e+00 -2.37800992e-01\n",
      "   0.00000000e+00]\n",
      " [ 1.81180619e+01 -3.09096431e+00  2.86995396e+01 -3.06029803e-01\n",
      "   0.00000000e+00]\n",
      " [-8.46729368e+00  2.27888324e+01 -4.62577019e+00  2.48485626e+01\n",
      "   2.00000000e+00]\n",
      " [ 1.89518236e+01  2.64495094e+01  2.39530255e+01  2.83933501e+01\n",
      "   2.00000000e+00]\n",
      " [-3.48784626e+01  2.67261133e+01 -3.05393696e+01  2.86154334e+01\n",
      "   2.00000000e+00]\n",
      " [ 5.25055809e+00  2.63793197e+01  1.02695026e+01  2.84618466e+01\n",
      "   2.00000000e+00]\n",
      " [ 2.67840713e+01 -3.15827703e+00  3.27198403e+01 -5.50086638e-01\n",
      "   2.00000000e+00]\n",
      " [-8.05685547e+00 -3.02108314e+00 -3.39664730e+00 -1.05141765e+00\n",
      "   2.00000000e+00]\n",
      " [-1.51386673e+01  2.64094474e+01 -1.07884090e+01  2.83737620e+01\n",
      "   2.00000000e+00]\n",
      " [-2.14512715e+01 -3.11078157e+00 -1.64856691e+01 -1.02472716e+00\n",
      "   2.00000000e+00]\n",
      " [-1.60810737e+00  2.64716945e+01  3.48713973e+00  2.84367455e+01\n",
      "   2.00000000e+00]\n",
      " [-1.46998493e+01 -3.22831736e+00 -8.40064202e+00 -7.88978605e-01\n",
      "   2.00000000e+00]\n",
      " [ 1.05023147e+01  6.81878332e+00  1.48814463e+01  8.78572735e+00\n",
      "   2.00000000e+00]\n",
      " [-2.78289321e+01 -3.01415591e+00 -2.21871237e+01 -7.10672028e-01\n",
      "   0.00000000e+00]\n",
      " [ 1.14088176e+01  2.63736241e+01  1.64352510e+01  2.85065533e+01\n",
      "   2.00000000e+00]\n",
      " [ 2.77546081e+01  2.64294116e+01  3.24667297e+01  2.83345947e+01\n",
      "   2.00000000e+00]\n",
      " [-2.89237825e+01  2.65495109e+01 -2.44486933e+01  2.84381484e+01\n",
      "   2.00000000e+00]\n",
      " [-1.12348084e+01  3.65351430e+00 -6.62355101e+00  5.71585118e+00\n",
      "   2.00000000e+00]\n",
      " [-2.38718599e+01  2.86119607e-02 -1.92615471e+01  2.03398033e+00\n",
      "   2.00000000e+00]\n",
      " [ 9.08817135e+00 -3.17211753e+00  1.39289442e+01 -1.03700430e+00\n",
      "   2.00000000e+00]\n",
      " [-2.21930425e+01  2.65014979e+01 -1.73169427e+01  2.83980668e+01\n",
      "   2.00000000e+00]]\n",
      "----------------------------load_annotations---------------------------------\n",
      "After input annotations(106,0) shape: (24, 5)\n",
      "Value\n",
      "[[ 1.53737541e+01 -3.04384770e+00  1.05857146e+01  2.80183481e+00\n",
      "   0.00000000e+00]\n",
      " [-1.16432906e+00 -2.99117800e+00  6.77166315e+00  2.76296797e+00\n",
      "   0.00000000e+00]\n",
      " [-3.76731628e+01  2.66910271e+01  4.34216478e+00  1.89630119e+00\n",
      "   2.00000000e+00]\n",
      " [-2.36605227e+01  2.57488874e-02  4.61335823e+00  2.01269773e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.61581607e+01  2.64999963e+01  5.00436004e+00  1.95187975e+00\n",
      "   2.00000000e+00]\n",
      " [-2.49868736e+01  2.64861887e+01  4.87918113e+00  1.90440696e+00\n",
      "   2.00000000e+00]\n",
      " [ 3.26935670e+01 -3.28202040e+00  9.21923919e+00  2.87663472e+00\n",
      "   0.00000000e+00]\n",
      " [ 3.33284984e+01  2.62210292e+01  4.85774365e+00  1.79036796e+00\n",
      "   2.00000000e+00]\n",
      " [ 2.40390375e+01 -3.09639014e+00  5.93210026e+00  2.59905556e+00\n",
      "   2.00000000e+00]\n",
      " [-1.74444365e+01 -3.23322725e+00  6.30311895e+00  2.44944483e+00\n",
      "   2.00000000e+00]\n",
      " [-4.40188141e+00  2.64893400e+01  5.09843950e+00  1.97324076e+00\n",
      "   2.00000000e+00]\n",
      " [-1.03742458e+01  3.66596847e+00  4.61439606e+00  2.06966692e+00\n",
      "   2.00000000e+00]\n",
      " [ 8.61482413e+00  2.64125094e+01  5.02905743e+00  2.14070452e+00\n",
      "   2.00000000e+00]\n",
      " [-3.05734982e+01 -3.04047863e+00  5.63846242e+00  2.29472571e+00\n",
      "   0.00000000e+00]\n",
      " [-1.79325107e+01  2.64056076e+01  4.35345263e+00  1.97131579e+00\n",
      "   2.00000000e+00]\n",
      " [-3.17181382e+01  2.65239026e+01  4.47815925e+00  1.89583545e+00\n",
      "   2.00000000e+00]\n",
      " [ 2.49605958e+01  2.64944079e+01  4.71521773e+00  1.91275973e+00\n",
      "   2.00000000e+00]\n",
      " [ 2.45645924e+00  2.64084510e+01  5.01636231e+00  2.07489417e+00\n",
      "   2.00000000e+00]\n",
      " [ 6.34239690e+00 -3.13775925e+00  4.84339514e+00  2.14257734e+00\n",
      "   2.00000000e+00]\n",
      " [-1.11632848e+01  2.64924531e+01  4.56256522e+00  2.00212481e+00\n",
      "   2.00000000e+00]\n",
      " [-1.08021680e+01 -3.01493646e+00  4.66310886e+00  1.97704309e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.12168428e+01  6.84708999e+00  4.37643285e+00  1.96024060e+00\n",
      "   2.00000000e+00]\n",
      " [-1.38089358e+01  2.27749429e+01  3.84485099e+00  2.06590980e+00\n",
      "   2.00000000e+00]\n",
      " [-2.41967726e+01 -3.12571626e+00  4.96895282e+00  2.09402473e+00\n",
      "   2.00000000e+00]]\n",
      "After transform annotations(106,0) shape: (24, 5)\n",
      "Value\n",
      "[[ 1.53737541e+01 -3.04384770e+00  2.59594688e+01 -2.42012889e-01\n",
      "   0.00000000e+00]\n",
      " [-1.16432906e+00 -2.99117800e+00  5.60733409e+00 -2.28210027e-01\n",
      "   0.00000000e+00]\n",
      " [-3.76731628e+01  2.66910271e+01 -3.33309980e+01  2.85873283e+01\n",
      "   2.00000000e+00]\n",
      " [-2.36605227e+01  2.57488874e-02 -1.90471645e+01  2.03844662e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.61581607e+01  2.64999963e+01  2.11625208e+01  2.84518760e+01\n",
      "   2.00000000e+00]\n",
      " [-2.49868736e+01  2.64861887e+01 -2.01076924e+01  2.83905957e+01\n",
      "   2.00000000e+00]\n",
      " [ 3.26935670e+01 -3.28202040e+00  4.19128062e+01 -4.05385679e-01\n",
      "   0.00000000e+00]\n",
      " [ 3.33284984e+01  2.62210292e+01  3.81862421e+01  2.80113972e+01\n",
      "   2.00000000e+00]\n",
      " [ 2.40390375e+01 -3.09639014e+00  2.99711377e+01 -4.97334582e-01\n",
      "   2.00000000e+00]\n",
      " [-1.74444365e+01 -3.23322725e+00 -1.11413176e+01 -7.83782421e-01\n",
      "   2.00000000e+00]\n",
      " [-4.40188141e+00  2.64893400e+01  6.96558094e-01  2.84625808e+01\n",
      "   2.00000000e+00]\n",
      " [-1.03742458e+01  3.66596847e+00 -5.75984972e+00  5.73563538e+00\n",
      "   2.00000000e+00]\n",
      " [ 8.61482413e+00  2.64125094e+01  1.36438816e+01  2.85532139e+01\n",
      "   2.00000000e+00]\n",
      " [-3.05734982e+01 -3.04047863e+00 -2.49350358e+01 -7.45752920e-01\n",
      "   0.00000000e+00]\n",
      " [-1.79325107e+01  2.64056076e+01 -1.35790580e+01  2.83769234e+01\n",
      "   2.00000000e+00]\n",
      " [-3.17181382e+01  2.65239026e+01 -2.72399789e+01  2.84197380e+01\n",
      "   2.00000000e+00]\n",
      " [ 2.49605958e+01  2.64944079e+01  2.96758135e+01  2.84071676e+01\n",
      "   2.00000000e+00]\n",
      " [ 2.45645924e+00  2.64084510e+01  7.47282155e+00  2.84833451e+01\n",
      "   2.00000000e+00]\n",
      " [ 6.34239690e+00 -3.13775925e+00  1.11857920e+01 -9.95181914e-01\n",
      "   2.00000000e+00]\n",
      " [-1.11632848e+01  2.64924531e+01 -6.60071957e+00  2.84945779e+01\n",
      "   2.00000000e+00]\n",
      " [-1.08021680e+01 -3.01493646e+00 -6.13905919e+00 -1.03789337e+00\n",
      "   2.00000000e+00]\n",
      " [ 1.12168428e+01  6.84708999e+00  1.55932756e+01  8.80733058e+00\n",
      "   2.00000000e+00]\n",
      " [-1.38089358e+01  2.27749429e+01 -9.96408484e+00  2.48408527e+01\n",
      "   2.00000000e+00]\n",
      " [-2.41967726e+01 -3.12571626e+00 -1.92278197e+01 -1.03169153e+00\n",
      "   2.00000000e+00]]\n",
      "tensor(125056.8750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.)\n",
      "checkpoint...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    last_epoch = step // num_iter_per_epoch\n",
    "    if epoch < last_epoch:\n",
    "        continue\n",
    "\n",
    "\n",
    "    epoch_loss = []\n",
    "    progress_bar = tqdm(training_generator)\n",
    "    for iter, data in enumerate(progress_bar):\n",
    "        if iter < step - last_epoch * num_iter_per_epoch:\n",
    "            progress_bar.update()\n",
    "            continue\n",
    "            \n",
    "        imgs = data['img']\n",
    "        annot = data['annot']\n",
    "\n",
    "#         sample_cat, sample, target, road_image, extra = data     \n",
    "#         sample_cat = torch.stack(sample_cat)\n",
    "\n",
    "        if params.num_gpus == 1:\n",
    "            # if only one gpu, just send it to cuda:0\n",
    "            # elif multiple gpus, send it to multiple gpus in CustomDataParallel, not here\n",
    "            imgs = imgs.cuda()\n",
    "            annot = annot.cuda()\n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "#         criterion = FocalLoss()\n",
    "#         _, regression, classification, anchors = model(imgs)\n",
    "#         cls_loss, reg_loss = criterion(classification, regression, anchors, annot)\n",
    "\n",
    "        \n",
    "        cls_loss, reg_loss, regression, classification, anchors = model(imgs, annot, obj_list=params.obj_list)\n",
    "#         cls_loss, reg_loss = model(imgs, target, obj_list=params.obj_list)\n",
    "\n",
    "\n",
    "        cls_loss = cls_loss.mean()\n",
    "        reg_loss = reg_loss.mean()\n",
    "\n",
    "\n",
    "        print(cls_loss)\n",
    "        print(reg_loss)\n",
    "\n",
    "        loss = cls_loss + reg_loss\n",
    "        if loss == 0 or not torch.isfinite(loss):\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(float(loss))\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            'Step: {}. Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Total loss: {:.5f}'.format(\n",
    "                step, epoch, 10, iter + 1, num_iter_per_epoch, cls_loss.item(),\n",
    "                reg_loss.item(), loss.item()))\n",
    "        writer.add_scalars('Loss', {'train': loss}, step)\n",
    "        writer.add_scalars('Regression_loss', {'train': reg_loss}, step)\n",
    "        writer.add_scalars('Classfication_loss', {'train': cls_loss}, step)\n",
    "\n",
    "        \n",
    "        # log learning_rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('learning_rate', current_lr, step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step % 1 == 0 and step > 0:\n",
    "            save_checkpoint(model, f'efficientdet-d{0}_{epoch}_{step}.pth')\n",
    "            print('checkpoint...')\n",
    "            \n",
    "            \n",
    "    \n",
    "        break\n",
    "    \n",
    "    break\n",
    "    scheduler.step(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if epoch % opt.val_interval == 0:\n",
    "#     model.eval()\n",
    "#     loss_regression_ls = []\n",
    "#     loss_classification_ls = []\n",
    "#     for iter, data in enumerate(val_generator):\n",
    "#         with torch.no_grad():\n",
    "#             imgs = data['img']\n",
    "#             annot = data['annot']\n",
    "\n",
    "#             if params.num_gpus == 1:\n",
    "#                 imgs = imgs.cuda()\n",
    "#                 annot = annot.cuda()\n",
    "\n",
    "#             cls_loss, reg_loss, regression, classification, anchors = model(imgs, annot, obj_list=params.obj_list)\n",
    "#             cls_loss = cls_loss.mean()\n",
    "#             reg_loss = reg_loss.mean()\n",
    "\n",
    "#             loss = cls_loss + reg_loss\n",
    "#             if loss == 0 or not torch.isfinite(loss):\n",
    "#                 continue\n",
    "\n",
    "#             loss_classification_ls.append(cls_loss.item())\n",
    "#             loss_regression_ls.append(reg_loss.item())\n",
    "\n",
    "#     cls_loss = np.mean(loss_classification_ls)\n",
    "#     reg_loss = np.mean(loss_regression_ls)\n",
    "#     loss = cls_loss + reg_loss\n",
    "\n",
    "#     print(\n",
    "#         'Val. Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}'.format(\n",
    "#             epoch, opt.num_epochs, cls_loss, reg_loss, loss))\n",
    "#     writer.add_scalars('Total_loss', {'val': loss}, step)\n",
    "#     writer.add_scalars('Regression_loss', {'val': reg_loss}, step)\n",
    "#     writer.add_scalars('Classfication_loss', {'val': cls_loss}, step)\n",
    "\n",
    "#     if loss + opt.es_min_delta < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_epoch = epoch\n",
    "\n",
    "#         save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "\n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49104, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49104, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49104, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49104, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = anchors[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = classification[0, :, :]\n",
    "regression = regression[0, :, :]\n",
    "\n",
    "bbox_annotation = annot[0]\n",
    "bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     for epoch in range(10):\n",
    "#         last_epoch = step // num_iter_per_epoch\n",
    "#         if epoch < last_epoch:\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         epoch_loss = []\n",
    "#         progress_bar = tqdm(training_generator)\n",
    "#         for iter, data in enumerate(progress_bar):\n",
    "#             if iter < step - last_epoch * num_iter_per_epoch:\n",
    "#                 progress_bar.update()\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 # imgs = data['img']\n",
    "#                 # annot = data['annot']\n",
    "\n",
    "#                 sample_cat, sample, target, road_image, extra = data\n",
    "\n",
    "#                 if params.num_gpus == 1:\n",
    "#                     # if only one gpu, just send it to cuda:0\n",
    "#                     # elif multiple gpus, send it to multiple gpus in CustomDataParallel, not here\n",
    "#                     imgs = imgs.cuda()\n",
    "#                     annot = annot.cuda()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 # cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n",
    "#                 cls_loss, reg_loss = model(sample_cat, target, obj_list=params.obj_list)\n",
    "#                 cls_loss = cls_loss.mean()\n",
    "#                 reg_loss = reg_loss.mean()\n",
    "\n",
    "#                 loss = cls_loss + reg_loss\n",
    "#                 if loss == 0 or not torch.isfinite(loss):\n",
    "#                     continue\n",
    "\n",
    "#                 loss.backward()\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 epoch_loss.append(float(loss))\n",
    "\n",
    "#                 progress_bar.set_description(\n",
    "#                     'Step: {}. Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Total loss: {:.5f}'.format(\n",
    "#                         step, epoch, opt.num_epochs, iter + 1, num_iter_per_epoch, cls_loss.item(),\n",
    "#                         reg_loss.item(), loss.item()))\n",
    "#                 writer.add_scalars('Loss', {'train': loss}, step)\n",
    "#                 writer.add_scalars('Regression_loss', {'train': reg_loss}, step)\n",
    "#                 writer.add_scalars('Classfication_loss', {'train': cls_loss}, step)\n",
    "\n",
    "#                 # log learning_rate\n",
    "#                 current_lr = optimizer.param_groups[0]['lr']\n",
    "#                 writer.add_scalar('learning_rate', current_lr, step)\n",
    "\n",
    "#                 step += 1\n",
    "\n",
    "#                 if step % opt.save_interval == 0 and step > 0:\n",
    "#                     save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "#                     print('checkpoint...')\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print('[Error]', traceback.format_exc())\n",
    "#                 print(e)\n",
    "#                 continue\n",
    "#         scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "#         if epoch % opt.val_interval == 0:\n",
    "#             model.eval()\n",
    "#             loss_regression_ls = []\n",
    "#             loss_classification_ls = []\n",
    "#             for iter, data in enumerate(val_generator):\n",
    "#                 with torch.no_grad():\n",
    "#                     imgs = data['img']\n",
    "#                     annot = data['annot']\n",
    "\n",
    "#                     if params.num_gpus == 1:\n",
    "#                         imgs = imgs.cuda()\n",
    "#                         annot = annot.cuda()\n",
    "\n",
    "#                     cls_loss, reg_loss = model(imgs, annot, obj_list=params.obj_list)\n",
    "#                     cls_loss = cls_loss.mean()\n",
    "#                     reg_loss = reg_loss.mean()\n",
    "\n",
    "#                     loss = cls_loss + reg_loss\n",
    "#                     if loss == 0 or not torch.isfinite(loss):\n",
    "#                         continue\n",
    "\n",
    "#                     loss_classification_ls.append(cls_loss.item())\n",
    "#                     loss_regression_ls.append(reg_loss.item())\n",
    "\n",
    "#             cls_loss = np.mean(loss_classification_ls)\n",
    "#             reg_loss = np.mean(loss_regression_ls)\n",
    "#             loss = cls_loss + reg_loss\n",
    "\n",
    "#             print(\n",
    "#                 'Val. Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}'.format(\n",
    "#                     epoch, opt.num_epochs, cls_loss, reg_loss, loss))\n",
    "#             writer.add_scalars('Total_loss', {'val': loss}, step)\n",
    "#             writer.add_scalars('Regression_loss', {'val': reg_loss}, step)\n",
    "#             writer.add_scalars('Classfication_loss', {'val': cls_loss}, step)\n",
    "\n",
    "#             if loss + opt.es_min_delta < best_loss:\n",
    "#                 best_loss = loss\n",
    "#                 best_epoch = epoch\n",
    "\n",
    "#                 save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "\n",
    "#             model.train()\n",
    "\n",
    "#             # Early stopping\n",
    "#             if epoch - best_epoch > opt.es_patience > 0:\n",
    "#                 print('[Info] Stop training at epoch {}. The lowest loss achieved is {}'.format(epoch, loss))\n",
    "#                 break\n",
    "# except KeyboardInterrupt:\n",
    "#     save_checkpoint(model, f'efficientdet-d{opt.compound_coef}_{epoch}_{step}.pth')\n",
    "#     writer.close()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
